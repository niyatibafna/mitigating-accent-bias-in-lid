{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(\"/home/hltcoe/nbafna/projects/mitigating-accent-bias-in-lid/utils/misc/\")\n",
    "from get_client_id_from_audio_file import get_speaker_id_from_audio_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_from_hf_l2_accents = {\"de_Amerikanisches Deutsch\",\n",
    "        \"de_Brasilianisches Deutsch\",\n",
    "        \"de_Britisches Deutsch\",\n",
    "        \"de_Niederländisch Deutsch\",\n",
    "        \"de_Französisch Deutsch\",\n",
    "        \"de_Kanadisches Deutsch\",\n",
    "        \"de_Italienisch Deutsch\",\n",
    "        \"de_Polnisch Deutsch\",\n",
    "        \"de_Russisch Deutsch\",\n",
    "        \"de_Tschechisch Deutsch\",\n",
    "        \"fr_Français de Roumanie\",\n",
    "        \"fr_Français d’Autriche\",\n",
    "        \"fr_Français d’Italie\",\n",
    "        \"fr_Français des États-Unis\",\n",
    "        \"fr_Français du Royaume-Uni\",\n",
    "        \"fr_Français d’Allemagne\"}\n",
    "\n",
    "cv_l1_accents = {\"canada\", \"us\", \"england\", \"australia\", \"newzealand\", \"scotland\", \"wales\", \"ireland\"}\n",
    "\n",
    "edacc_l1_accents = {\"us\", \"uk\", \"irish\", \"scottish\", \"american\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_edacc_results(predictions_dir):\n",
    "    \n",
    "    predictions_path = predictions_dir + \"/edacc_predictions.pkl\"\n",
    "    if not os.path.exists(predictions_path):\n",
    "        predictions_path = predictions_dir + \"/predictions.pkl\"\n",
    "\n",
    "    if not os.path.exists(predictions_path):\n",
    "        print(f\"No predictions found in the specified directory: {predictions_dir}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    with open(predictions_path, \"rb\") as f:\n",
    "        eval_data = pkl.load(f)\n",
    "\n",
    "    results_by_accent = defaultdict(lambda: defaultdict(int))\n",
    "    # print(list(zip(eval_data[\"preds\"], eval_data[\"accents\"], eval_data[\"labels\"])))\n",
    "    for prediction, accent, label in zip(eval_data[\"preds\"], eval_data[\"accents\"], eval_data[\"labels\"]):\n",
    "        if accent in edacc_l1_accents:\n",
    "            continue\n",
    "        if prediction == label:\n",
    "            results_by_accent[accent][\"correct\"] += 1\n",
    "        results_by_accent[accent][\"total\"] += 1\n",
    "    # %%\n",
    "\n",
    "    # # Merge \"us\" and \"american\"  accents\n",
    "    # results_by_accent[\"us\"] = {k: results_by_accent[\"us\"].get(k, 0) + results_by_accent[\"american\"].get(k, 0) for k in set(results_by_accent[\"us\"]) | set(results_by_accent[\"american\"])}\n",
    "    # # results_by_accent[\"us\"][\"total\"] = sum([results_by_accent[\"us\"][\"total\"], results_by_accent[\"american\"][\"total\"]])\n",
    "    # del results_by_accent[\"american\"]\n",
    "\n",
    "    # Compute macro-average accuracy, ignoring accents with < 10 samples\n",
    "    acc_by_accent = {accent: results_by_accent[accent][\"correct\"]/results_by_accent[accent][\"total\"] \\\n",
    "                        for accent in results_by_accent}\n",
    "\n",
    "    macro_avg = np.mean([acc for accent, acc in acc_by_accent.items() if results_by_accent[accent][\"total\"] >= 10])\n",
    "    micro_avg = sum([results_by_accent[accent][\"correct\"] for accent in results_by_accent]) \\\n",
    "        / sum([results_by_accent[accent][\"total\"] for accent in results_by_accent])\n",
    "    std_dev = np.std([acc for accent, acc in acc_by_accent.items() if results_by_accent[accent][\"total\"] >= 10])\n",
    "    \n",
    "    return results_by_accent, macro_avg, micro_avg, std_dev\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_cv_results(predictions_dir):\n",
    "    predictions_path = predictions_dir + \"/cv_predictions.pkl\"\n",
    "    with open(predictions_path, \"rb\") as f:\n",
    "        eval_data = pkl.load(f)\n",
    "\n",
    "    results_by_accent = defaultdict(lambda: defaultdict(int))\n",
    "    # print(list(zip(eval_data[\"preds\"], eval_data[\"accents\"], eval_data[\"labels\"])))\n",
    "    for prediction, accent, label in zip(eval_data[\"preds\"], eval_data[\"accents\"], eval_data[\"labels\"]):\n",
    "        if accent in cv_l1_accents:\n",
    "            continue\n",
    "        if prediction == label:\n",
    "            results_by_accent[accent][\"correct\"] += 1\n",
    "        results_by_accent[accent][\"total\"] += 1\n",
    "\n",
    "    # Compute macro-average accuracy, ignoring accents with < 10 samples\n",
    "    acc_by_accent = {accent: results_by_accent[accent][\"correct\"]/results_by_accent[accent][\"total\"] \\\n",
    "                        for accent in results_by_accent}\n",
    "\n",
    "    macro_avg = np.mean([acc for accent, acc in acc_by_accent.items() if results_by_accent[accent][\"total\"] >= 10])\n",
    "    micro_avg = sum([results_by_accent[accent][\"correct\"] for accent in results_by_accent]) \\\n",
    "        / sum([results_by_accent[accent][\"total\"] for accent in results_by_accent])\n",
    "    std_dev = np.std([acc for accent, acc in acc_by_accent.items() if results_by_accent[accent][\"total\"] >= 10])\n",
    "    \n",
    "    return results_by_accent, macro_avg, micro_avg, std_dev\n",
    "\n",
    "\n",
    "\n",
    "def read_cv_from_hf_results(predictions_dir):\n",
    "    predictions_path = predictions_dir + \"/cv_from_hf_predictions.pkl\"\n",
    "    with open(predictions_path, \"rb\") as f:\n",
    "        eval_data = pkl.load(f)\n",
    "\n",
    "    results_by_accent = defaultdict(lambda: defaultdict(int))\n",
    "    # print(list(zip(eval_data[\"preds\"], eval_data[\"accents\"], eval_data[\"labels\"])))\n",
    "    for prediction, accent, label in zip(eval_data[\"preds\"], eval_data[\"accents\"], eval_data[\"labels\"]):\n",
    "        if prediction == label:\n",
    "            results_by_accent[accent][\"correct\"] += 1\n",
    "        results_by_accent[accent][\"total\"] += 1\n",
    "\n",
    "    # Compute macro-average accuracy, ignoring accents with < 10 samples\n",
    "    acc_by_accent = {accent: results_by_accent[accent][\"correct\"]/results_by_accent[accent][\"total\"] \\\n",
    "                        for accent in results_by_accent}\n",
    "\n",
    "    macro_avg = np.mean([acc for accent, acc in acc_by_accent.items() if results_by_accent[accent][\"total\"] >= 10])\n",
    "    micro_avg = sum([results_by_accent[accent][\"correct\"] for accent in results_by_accent]) \\\n",
    "        / sum([results_by_accent[accent][\"total\"] for accent in results_by_accent])\n",
    "    std_dev = np.std([acc for accent, acc in acc_by_accent.items() if results_by_accent[accent][\"total\"] >= 10])\n",
    "    \n",
    "    return results_by_accent, macro_avg, micro_avg, std_dev\n",
    "\n",
    "\n",
    "def read_cv_from_hf_l2_results(predictions_dir):\n",
    "    predictions_path = predictions_dir + \"/cv_from_hf_predictions.pkl\"\n",
    "    with open(predictions_path, \"rb\") as f:\n",
    "        eval_data = pkl.load(f)\n",
    "\n",
    "    results_by_accent = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # print(list(zip(eval_data[\"preds\"], eval_data[\"accents\"], eval_data[\"labels\"])))\n",
    "    for prediction, accent, label in zip(eval_data[\"preds\"], eval_data[\"accents\"], eval_data[\"labels\"]):\n",
    "        if f\"{label}_{accent.strip()}\" not in cv_from_hf_l2_accents:\n",
    "            continue\n",
    "        if prediction == label:\n",
    "            results_by_accent[accent][\"correct\"] += 1\n",
    "        results_by_accent[accent][\"total\"] += 1\n",
    "\n",
    "    # Compute macro-average accuracy, ignoring accents with < 10 samples\n",
    "    acc_by_accent = {accent: results_by_accent[accent][\"correct\"]/results_by_accent[accent][\"total\"] \\\n",
    "                        for accent in results_by_accent}\n",
    "\n",
    "    macro_avg = np.mean([acc for accent, acc in acc_by_accent.items() if results_by_accent[accent][\"total\"] >= 10])\n",
    "    micro_avg = sum([results_by_accent[accent][\"correct\"] for accent in results_by_accent]) \\\n",
    "        / sum([results_by_accent[accent][\"total\"] for accent in results_by_accent])\n",
    "    std_dev = np.std([acc for accent, acc in acc_by_accent.items() if results_by_accent[accent][\"total\"] >= 10])\n",
    "    \n",
    "    return results_by_accent, macro_avg, micro_avg, std_dev\n",
    "\n",
    "\n",
    "def read_fleurs_results(predictions_dir):\n",
    "\n",
    "    predictions_path = predictions_dir + \"/fleurs_test_predictions.pkl\"\n",
    "\n",
    "    with open(predictions_path, \"rb\") as f:\n",
    "        eval_data = pkl.load(f)\n",
    "\n",
    "    results_by_lang = defaultdict(lambda: defaultdict(int))\n",
    "    # print(list(zip(eval_data[\"preds\"], eval_data[\"accents\"], eval_data[\"labels\"])))\n",
    "    for prediction, accent, label in zip(eval_data[\"preds\"], eval_data[\"accents\"], eval_data[\"labels\"]):\n",
    "        if prediction == label:\n",
    "            results_by_lang[label][\"correct\"] += 1\n",
    "        results_by_lang[label][\"total\"] += 1\n",
    "    \n",
    "    # Compute macro-average accuracy, ignoring accents with < 10 samples\n",
    "    acc_by_accent = {lang: results_by_lang[lang][\"correct\"]/results_by_lang[lang][\"total\"] \\\n",
    "                        for lang in results_by_lang}\n",
    "\n",
    "    macro_avg = np.mean([acc for lang, acc in acc_by_accent.items() if results_by_lang[lang][\"total\"] >= 10])\n",
    "    micro_avg = sum([results_by_lang[lang][\"correct\"] for lang in results_by_lang]) \\\n",
    "        / sum([results_by_lang[lang][\"total\"] for lang in results_by_lang])\n",
    "    std_dev = np.std([acc for lang, acc in acc_by_accent.items() if results_by_lang[lang][\"total\"] >= 10])\n",
    "    \n",
    "    return results_by_lang, macro_avg, micro_avg, std_dev\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_percentage(x):\n",
    "    return round(x * 100, 1)\n",
    "\n",
    "def get_eval_data(dataset_name, predictions_dir, lang = None, accent = None):\n",
    "\n",
    "    if dataset_name == \"edacc\":\n",
    "        predictions_path = predictions_dir + \"/edacc_predictions.pkl\"\n",
    "        if not os.path.exists(predictions_path):\n",
    "            predictions_path = predictions_dir + \"/predictions.pkl\"\n",
    "\n",
    "        if not os.path.exists(predictions_path):\n",
    "            print(f\"No predictions found in the specified directory: {predictions_dir}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    elif dataset_name == \"cv\":\n",
    "        predictions_path = predictions_dir + \"/cv_predictions.pkl\"\n",
    "\n",
    "\n",
    "    elif dataset_name in {\"cv_from_hf\", \"cv_from_hf_l2\"}:\n",
    "        predictions_path = predictions_dir + \"/cv_from_hf_predictions.pkl\"\n",
    "\n",
    "\n",
    "    elif dataset_name == \"fleurs_test\":\n",
    "        predictions_path = predictions_dir + \"/fleurs_test_predictions.pkl\"\n",
    "\n",
    "    with open(predictions_path, \"rb\") as f:\n",
    "        eval_data = pkl.load(f)\n",
    "    \n",
    "    \n",
    "    data = []\n",
    "    for pred, label, accent, audio_file in zip(eval_data[\"preds\"], eval_data[\"labels\"], eval_data[\"accents\"], eval_data[\"audio_files\"]):\n",
    "        if lang is not None and label != lang:\n",
    "            continue\n",
    "        if dataset_name == \"cv_from_hf_l2\":\n",
    "            if f\"{label}_{accent.strip()}\" not in cv_from_hf_l2_accents:\n",
    "                continue\n",
    "        if dataset_name == \"cv\":\n",
    "            if accent in cv_l1_accents:\n",
    "                continue\n",
    "        if dataset_name == \"edacc\":\n",
    "            if accent in edacc_l1_accents:\n",
    "                continue\n",
    "        speaker_id = get_speaker_id_from_audio_file(dataset_name, audio_file=audio_file, accent=accent, lang=label)\n",
    "        data.append((pred, label, speaker_id))\n",
    "\n",
    "    # print(f\"Number of samples in the evaluation data: {len(data)}\")\n",
    "    # print(f\"Number of speakers in the evaluation data: {len(set([x[2] for x in data]))}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def compute_accuracy(pred_labels):\n",
    "    \"\"\"Computes accuracy given a list of (pred, label) tuples.\"\"\"\n",
    "    correct = sum(1 for pred, label in pred_labels if pred == label)\n",
    "    return correct / len(pred_labels) if pred_labels else 0.0\n",
    "\n",
    "def calculate_bootstrap_ci(system_data, n_bootstraps=1000, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Computes bootstrap confidence interval for the accuracy of a single LID system.\n",
    "    \n",
    "    :param system_data: List of (pred, label, speaker_id) tuples\n",
    "    :param n_bootstraps: Number of bootstrap resamples\n",
    "    :param confidence: Confidence level (e.g., 0.95 for 95% CI)\n",
    "    :return: (lower_bound, upper_bound) of the accuracy CI\n",
    "    \"\"\"\n",
    "    \n",
    "    # Organize data by speaker\n",
    "    speaker_data = {}\n",
    "    for pred, label, speaker in system_data:\n",
    "        if speaker not in speaker_data:\n",
    "            speaker_data[speaker] = []\n",
    "        speaker_data[speaker].append((pred, label))  # Store predictions and labels per speaker\n",
    "    \n",
    "    speakers = list(speaker_data.keys())  # Unique speakers\n",
    "    boot_accuracies = []\n",
    "\n",
    "    # Bootstrap resampling\n",
    "    for _ in range(n_bootstraps):\n",
    "        resampled_speakers = np.random.choice(speakers, size=len(speakers), replace=True)\n",
    "        \n",
    "        # Collect resampled predictions\n",
    "        resampled_preds = []\n",
    "        for speaker in resampled_speakers:\n",
    "            resampled_preds.extend(speaker_data[speaker])\n",
    "\n",
    "        # Compute accuracy for the resampled dataset\n",
    "        boot_accuracies.append(compute_accuracy(resampled_preds))\n",
    "\n",
    "    mean_accuracy = np.mean(boot_accuracies)\n",
    "    lower = np.percentile(boot_accuracies, (1 - confidence) / 2 * 100)\n",
    "    upper = np.percentile(boot_accuracies, (1 + confidence) / 2 * 100)\n",
    "\n",
    "    return mean_accuracy, lower, upper\n",
    "\n",
    "\n",
    "def get_bootstrap_ci(dataset_name, predictions_dir, lang = None, accent = None):\n",
    "    '''\n",
    "    Should return a list of tuples [(pred, label, speaker_id)]\n",
    "    '''\n",
    "    \n",
    "    data = get_eval_data(dataset_name, predictions_dir)\n",
    "    mean_accuracy, lower, upper = calculate_bootstrap_ci(data)\n",
    "\n",
    "    return format_percentage(mean_accuracy), format_percentage(lower), format_percentage(upper)\n",
    "\n",
    "\n",
    "\n",
    "def compute_speaker_accuracy(system_data):\n",
    "    \"\"\"\n",
    "    Computes per-speaker accuracy.\n",
    "    \n",
    "    :param system_data: List of (pred, label, speaker_id) tuples\n",
    "    :return: Dictionary {speaker: accuracy}\n",
    "    \"\"\"\n",
    "    speaker_results = {}\n",
    "    for pred, label, speaker in system_data:\n",
    "        if speaker not in speaker_results:\n",
    "            speaker_results[speaker] = []\n",
    "        speaker_results[speaker].append(pred == label)  # Store binary correctness\n",
    "    \n",
    "    # Compute per-speaker accuracy\n",
    "    return {speaker: np.mean(correct_list) for speaker, correct_list in speaker_results.items()}\n",
    "\n",
    "def t_test_speaker_level(system1_data, system2_data):\n",
    "    \"\"\"\n",
    "    Perform an independent t-test on speaker-level accuracies.\n",
    "    \n",
    "    :param system1_data: List of (pred, label, speaker_id) tuples for System 1\n",
    "    :param system2_data: List of (pred, label, speaker_id) tuples for System 2\n",
    "    :return: t-statistic, p-value\n",
    "    \"\"\"\n",
    "    acc_sys1 = list(compute_speaker_accuracy(system1_data).values())\n",
    "    acc_sys2 = list(compute_speaker_accuracy(system2_data).values())\n",
    "\n",
    "    # Welch's t-test (assumes unequal variances)\n",
    "    t_stat, p_value = ttest_ind(acc_sys1, acc_sys2, equal_var=False)\n",
    "    \n",
    "    return t_stat, p_value\n",
    "\n",
    "\n",
    "\n",
    "def compute_samplewise_correctness(preds, labels):\n",
    "    \"\"\"Converts (pred, label) pairs into binary correctness (1 for correct, 0 for incorrect).\"\"\"\n",
    "    return [1 if pred == label else 0 for pred, label in zip(preds, labels)]\n",
    "\n",
    "def paired_t_test(system1_data, system2_data):\n",
    "    \"\"\"\n",
    "    Performs a paired t-test on per-sample accuracy between two systems.\n",
    "    \n",
    "    :param system1_data: List of (pred, label) tuples for System 1\n",
    "    :param system2_data: List of (pred, label) tuples for System 2\n",
    "    :return: t-statistic, p-value\n",
    "    \"\"\"\n",
    "    assert len(system1_data) == len(system2_data), \"Both systems must have predictions for the same samples.\"\n",
    "    \n",
    "    # Extract predictions and labels\n",
    "    preds1, labels1 = zip(*system1_data)\n",
    "    preds2, labels2 = zip(*system2_data)\n",
    "\n",
    "    # Convert to binary correctness (1 = correct, 0 = incorrect)\n",
    "    correct1 = compute_samplewise_correctness(preds1, labels1)\n",
    "    correct2 = compute_samplewise_correctness(preds2, labels2)\n",
    "\n",
    "    # Perform a paired t-test (Welch’s version handles unequal variances)\n",
    "    t_stat, p_value = ttest_rel(correct1, correct2)\n",
    "\n",
    "    return t_stat, p_value\n",
    "\n",
    "\n",
    "def mcnemar_test(system1_data, system2_data):\n",
    "    \"\"\"\n",
    "    Perform McNemar's test to compare two classifiers' error distributions.\n",
    "    \n",
    "    :param system1_data: List of (pred, label) tuples for System 1\n",
    "    :param system2_data: List of (pred, label) tuples for System 2\n",
    "    :return: Chi-squared statistic, p-value\n",
    "    \"\"\"\n",
    "    assert len(system1_data) == len(system2_data), \"Both systems must have predictions for the same samples.\"\n",
    "    \n",
    "    # Initialize contingency table counts\n",
    "    A = B = C = D = 0\n",
    "    \n",
    "    for (pred1, label), (pred2, _) in zip(system1_data, system2_data):\n",
    "        correct1 = pred1 == label\n",
    "        correct2 = pred2 == label\n",
    "        \n",
    "        if correct1 and correct2:\n",
    "            A += 1  # Both correct\n",
    "        elif correct1 and not correct2:\n",
    "            B += 1  # System 1 correct, System 2 incorrect\n",
    "        elif not correct1 and correct2:\n",
    "            C += 1  # System 1 incorrect, System 2 correct\n",
    "        else:\n",
    "            D += 1  # Both incorrect\n",
    "    \n",
    "    # McNemar's test only uses B and C\n",
    "    contingency_table = [[A, B], [C, D]]\n",
    "    \n",
    "    chi2_stat, p_value, _, _ = chi2_contingency(contingency_table, correction=True)\n",
    "\n",
    "    return chi2_stat, p_value\n",
    "\n",
    "\n",
    "\n",
    "def significance_statistics(dataset_name, predictions_dir1, predictions_dir2, lang = None, accent = None, test_name = \"ttest\"):\n",
    "    '''\n",
    "    Should return a list of tuples [(pred, label, speaker_id)]\n",
    "    '''\n",
    "    \n",
    "    data1 = get_eval_data(dataset_name, predictions_dir1, lang, accent)\n",
    "    data2 = get_eval_data(dataset_name, predictions_dir2, lang, accent)\n",
    "    # print(f\"Number of speakers in system 1: {len(set([x[2] for x in data1]))}\")\n",
    "    # print(f\"Number of speakers in system 2: {len(set([x[2] for x in data2]))}\")\n",
    "\n",
    "    if test_name == \"ttest\":\n",
    "        stat, p_value = t_test_speaker_level(data1, data2)\n",
    "    \n",
    "    elif test_name == \"paired_ttest\":\n",
    "        # Let's sort both lists by speaker_id\n",
    "\n",
    "        data1 = sorted(data1, key=lambda x: x[2])\n",
    "        data2 = sorted(data2, key=lambda x: x[2])\n",
    "\n",
    "        assert [x[2] for x in data1] == [x[2] for x in data2], \"Speaker IDs do not match between the two systems.\"\n",
    "\n",
    "        data1 = [(x[0], x[1]) for x in data1]\n",
    "        data2 = [(x[0], x[1]) for x in data2]\n",
    "        stat, p_value = paired_t_test(data1, data2)\n",
    "\n",
    "    elif test_name == \"mcnemar\":\n",
    "        data1 = sorted(data1, key=lambda x: x[2])\n",
    "        data2 = sorted(data2, key=lambda x: x[2])\n",
    "\n",
    "        assert [x[2] for x in data1] == [x[2] for x in data2], \"Speaker IDs do not match between the two systems.\"\n",
    "\n",
    "        data1 = [(x[0], x[1]) for x in data1]\n",
    "        data2 = [(x[0], x[1]) for x in data2]\n",
    "\n",
    "        stat, p_value = mcnemar_test(data1, data2)\n",
    "\n",
    "    \n",
    "\n",
    "    return stat, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required format:\n",
    "\n",
    "# \t\t\t\t\t\t\tFLEURS\tEdAcc\tCV\n",
    "# ET\n",
    "# duseqs,w2v2-att4-1000\n",
    "# phoneseqs,vl107,att=8\n",
    "# ET+phoneseqs_combo\n",
    "# ET+duseq_train\n",
    "# ET+duseqembed\n",
    "# ET+phoneseqs_train\n",
    "\n",
    "\n",
    "approach2dir = {\n",
    "    \"ET\": \"/home/hltcoe/nbafna/projects/mitigating-accent-bias-in-lid/prelim_evals/preds/formatted\",\n",
    "    \"duseqs\": \"/exp/nbafna/projects/mitigating-accent-bias-in-lid/wav2vec2_intermediate_outputs/vl107/wav2vec2-base-layer8-1000/cnn-attentions-linear-4/lid_model_outputs/\",\n",
    "    \"ET+duseq-train\": \"/exp/nbafna/projects/mitigating-accent-bias-in-lid/reps_phoneseqs_duseqs_exps/vl107/ecapa-tdnn_wav2vec2-xlsr-53-espeak-cv-ft_wav2vec2-base-layer8-1000/attentions-linear-4/reps-phoneseq-duseqs_lid_model_outputs\",\n",
    "    \"ET+duseqembed-train\": \"/exp/nbafna/projects/mitigating-accent-bias-in-lid/reps_phoneseqs_duseqembed_exps/vl107/ecapa-tdnn_wav2vec2-xlsr-53-espeak-cv-ft_wav2vec2-base-layer8-1000/attentions-linear-4/reps-phoneseq-duseqs_lid_model_outputs\",\n",
    "    \"phoneseqs\": \"/exp/nbafna/projects/mitigating-accent-bias-in-lid/phoneseq_exps/vl107/wav2vec2-xlsr-53-espeak-cv-ft/attentions-linear-8/phoneseq_lid_model_outputs/\",\n",
    "    \"ET+phoneseqs\": \"/exp/nbafna/projects/mitigating-accent-bias-in-lid/dists-phoneseq-systemcombo_exps/vl107/ecapa-tdnn_wav2vec2-xlsr-53-espeak-cv-ft/attentions-linear-8/lid_model_outputs/\",\n",
    "    \"ET+phoneseqs-train\": \"/exp/nbafna/projects/mitigating-accent-bias-in-lid/reps-phoneseq_exps/vl107/ecapa-tdnn_wav2vec2-xlsr-53-espeak-cv-ft/attentions-linear-8/reps-phoneseq_lid_model_outputs/\",\n",
    "\n",
    "}\n",
    "\n",
    "dataset_to_processor = {\"fleurs_test\": read_fleurs_results, \"cv\": read_cv_results,\\\n",
    "                         \"edacc\": read_edacc_results, \"cv_from_hf\": read_cv_from_hf_results, \\\n",
    "                            \"cv_from_hf_l2\": read_cv_from_hf_l2_results}\n",
    "\n",
    "results = defaultdict(lambda: defaultdict(dict))\n",
    "for approach, dir in approach2dir.items():\n",
    "    # for dataset in [\"fleurs_test\", \"edacc\", \"cv\", \"cv_from_hf\", \"cv_from_hf_l2\"]:\n",
    "    for dataset in [\"fleurs_test\", \"edacc\", \"cv\", \"cv_from_hf_l2\"]:\n",
    "        results_by_variety, macro_avg, micro_avg, std_dev = dataset_to_processor[dataset](dir)\n",
    "        results[approach][dataset][\"macro_avg\"] = format_percentage(macro_avg)\n",
    "        results[approach][dataset][\"micro_avg\"] = format_percentage(micro_avg)\n",
    "        results[approach][dataset][\"std_dev\"] = format_percentage(std_dev)\n",
    "        mean_accuracy, lower, upper = get_bootstrap_ci(dataset, dir)\n",
    "        results[approach][dataset][\"bootstrap_mean\"] = mean_accuracy\n",
    "        results[approach][dataset][\"bootstrap_lower\"] = lower\n",
    "        results[approach][dataset][\"bootstrap_upper\"] = upper\n",
    "        # Significance test against ET for each dataset and approach\n",
    "        results[approach][dataset][\"paired_ttest_sig\"] = False\n",
    "        if approach == \"ET\":\n",
    "            continue\n",
    "        stat, p_value = significance_statistics(dataset, approach2dir[\"ET\"], dir, test_name=\"mcnemar\")\n",
    "        if p_value < 0.05:\n",
    "            results[approach][dataset][\"paired_ttest_sig\"] = True\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      " & fleurs_test & edacc & cv & cv_from_hf_l2 \\\\\n",
      "\\midrule\n",
      "\\et & 89.3 (89.0,89.6)  & 89.5±17.2 & 47.6 (39.8,55.6)  & 54.8±26.5 & 31.7 (22.5,43.4)  & 48.8±18.0 & 63.7 (54.0,73.1)  & 68.4±22.2 \\\\\n",
      "\\etpstrain & 86.6 (86.2,87.0)  & 86.4±18.2 & 57.3 (48.6,66.5)  & 63.4±25.5 & 68.3 (60.8,75.6)  & 80.6±11.4 & 73.0 (63.2,80.7)  & 76.0±13.9 \\\\\n",
      "\\etps & 89.5 (89.2,89.9)  & 89.5±17.8 & 52.0 (44.5,60.7)  & 59.0±26.4 & 44.5 (35.3,54.9)  & 63.8±15.6 & 66.8 (56.6,75.5)  & 73.6±20.3 \\\\\n",
      "\\phoneseq & 52.9 (52.1,53.7)  & 52.5±22.7 & 37.3 (30.5,44.1)  & 44.4±22.6 & 45.5 (39.0,53.1)  & 64.5±13.2 & 48.7 (40.4,56.2)  & 51.6±14.9 \\\\\n",
      "\\duseq & 49.6 (48.9,50.4)  & 49.8±18.3 & 42.5 (37.5,47.8)  & 48.4±18.0 & 47.0 (37.8,55.5)  & 63.1±12.5 & 48.0 (41.1,55.0)  & 48.2±19.2 \\\\\n",
      "\\etdutrain & 84.7 (84.2,85.1)  & 84.9±18.9 & 50.7 (43.0,57.9)  & 57.6±24.3 & 46.5 (38.0,56.3)  & 63.5±14.9 & 68.6 (60.1,76.2)  & 70.0±22.7 \\\\\n",
      "\\etduembedtrain & 84.2 (83.8,84.7)  & 84.2±20.2 & 53.2 (46.3,60.8)  & 60.0±23.6 & 50.0 (41.5,59.9)  & 64.1±11.9 & 63.8 (55.1,71.7)  & 65.6±22.8 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "approach2key = {\n",
    "    \"ET\": r\"\\et\",\n",
    "    \"duseqs\": r\"\\duseq\",\n",
    "    \"ET+duseq-train\": r\"\\etdutrain\",\n",
    "    \"ET+duseqembed-train\": r\"\\etduembedtrain\",\n",
    "    \"phoneseqs\": r\"\\phoneseq\",\n",
    "    \"ET+phoneseqs\": r\"\\etps\",\n",
    "    \"ET+phoneseqs-train\": r\"\\etpstrain\",\n",
    "}\n",
    "approaches = [\"ET\",  \"ET+phoneseqs-train\",  \"ET+phoneseqs\", \"phoneseqs\", \"duseqs\", \"ET+duseq-train\", \"ET+duseqembed-train\",]\n",
    "\n",
    "results_formatted = {approach2key[approach]: {dataset: f\"{results[approach][dataset][\"bootstrap_mean\"]} ({results[approach][dataset][\"bootstrap_lower\"]},{results[approach][dataset][\"bootstrap_upper\"]}){r\"$^\\dagger$\" if (approach != \"ET\" and not results[approach][dataset][\"paired_ttest_sig\"]) else \"\"}  & {results[approach][dataset][\"macro_avg\"]}{chr(177)}{results[approach][dataset][\"std_dev\"]}\" for dataset in results[approach]} for approach in approaches}\n",
    "df = pd.DataFrame(results_formatted)\n",
    "\n",
    "print(df.T.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllll}\n",
      "\\toprule\n",
      " & fleurs_test & edacc & cv & cv_from_hf_l2 \\\\\n",
      "\\midrule\n",
      "\\et & 89.3 (89.0,89.6)  & 89.5±17.2 & 47.7 (40.1,56.2)  & 55.8±26.8 & 33.9 (23.7,48.4)  & 57.0±24.6 & 63.6 (54.2,73.1)  & 68.4±22.2 \\\\\n",
      "\\etpstrain & 86.6 (86.1,87.0)  & 86.4±18.2 & 57.1 (48.9,65.6)  & 64.0±25.4 & 68.8 (61.2,76.1)  & 81.6±10.7 & 72.8 (63.3,81.0)  & 76.0±13.9 \\\\\n",
      "\\etps & 89.5 (89.2,89.9)$^\\dagger$  & 89.5±17.8 & 52.0 (43.9,60.5)  & 59.8±26.5 & 46.1 (37.3,57.2)  & 69.1±18.6 & 66.8 (56.6,75.7)  & 73.6±20.3 \\\\\n",
      "\\phoneseq & 52.9 (52.1,53.7)  & 52.5±22.7 & 37.3 (30.5,44.2)  & 45.2±22.8 & 47.1 (40.9,54.8)  & 67.3±13.6 & 48.8 (40.6,56.0)  & 51.6±14.9 \\\\\n",
      "\\duseq & 49.6 (48.9,50.3)  & 49.8±18.3 & 42.6 (37.3,48.0)  & 48.6±17.8 & 48.5 (39.4,56.7)  & 66.3±13.5 & 48.1 (40.9,55.2)  & 48.2±19.2 \\\\\n",
      "\\etdutrain & 84.7 (84.3,85.1)  & 84.9±18.9 & 50.7 (43.0,58.1)  & 58.3±24.4 & 47.8 (39.3,58.0)  & 67.0±15.6 & 68.4 (59.8,76.6)  & 70.0±22.7 \\\\\n",
      "\\etduembedtrain & 84.3 (83.8,84.7)  & 84.2±20.2 & 53.4 (45.9,60.9)  & 60.7±23.7 & 51.4 (43.5,60.7)  & 67.5±13.3 & 63.7 (54.8,71.3)$^\\dagger$  & 65.6±22.8 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "approach2key = {\n",
    "    \"ET\": r\"\\et\",\n",
    "    \"duseqs\": r\"\\duseq\",\n",
    "    \"ET+duseq-train\": r\"\\etdutrain\",\n",
    "    \"ET+duseqembed-train\": r\"\\etduembedtrain\",\n",
    "    \"phoneseqs\": r\"\\phoneseq\",\n",
    "    \"ET+phoneseqs\": r\"\\etps\",\n",
    "    \"ET+phoneseqs-train\": r\"\\etpstrain\",\n",
    "}\n",
    "approaches = [\"ET\",  \"ET+phoneseqs-train\",  \"ET+phoneseqs\", \"phoneseqs\", \"duseqs\", \"ET+duseq-train\", \"ET+duseqembed-train\",]\n",
    "\n",
    "results_formatted = {approach2key[approach]: {dataset: f\"{results[approach][dataset][\"micro_avg\"]} ({results[approach][dataset][\"bootstrap_lower\"]},{results[approach][dataset][\"bootstrap_upper\"]}){r\"$^\\dagger$\" if (approach != \"ET\" and not results[approach][dataset][\"paired_ttest_sig\"]) else \"\"}  & {results[approach][dataset][\"macro_avg\"]}{chr(177)}{results[approach][dataset][\"std_dev\"]}\" for dataset in results[approach]} for approach in approaches}\n",
    "df = pd.DataFrame(results_formatted)\n",
    "\n",
    "print(df.T.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_from_hf_l2 ET\n",
      "Mean accuracy: 70.7\n",
      "CI: (65.1, 76.0)\n",
      "\n",
      "\n",
      "cv_from_hf_l2 duseqs\n",
      "Mean accuracy: 50.4\n",
      "CI: (45.9, 55.0)\n",
      "\n",
      "\n",
      "cv_from_hf_l2 ET+duseq-train\n",
      "Mean accuracy: 71.6\n",
      "CI: (66.1, 76.9)\n",
      "\n",
      "\n",
      "cv_from_hf_l2 ET+duseqembed-train\n",
      "Mean accuracy: 69.3\n",
      "CI: (63.7, 74.3)\n",
      "\n",
      "\n",
      "cv_from_hf_l2 phoneseqs\n",
      "Mean accuracy: 49.1\n",
      "CI: (43.6, 54.5)\n",
      "\n",
      "\n",
      "cv_from_hf_l2 ET+phoneseqs\n",
      "Mean accuracy: 71.3\n",
      "CI: (65.3, 76.5)\n",
      "\n",
      "\n",
      "cv_from_hf_l2 ET+phoneseqs-train\n",
      "Mean accuracy: 74.8\n",
      "CI: (69.4, 80.1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "approach2dir = {\n",
    "    \"ET\": \"/home/hltcoe/nbafna/projects/mitigating-accent-bias-in-lid/prelim_evals/preds/formatted\",\n",
    "    \"duseqs\": \"/exp/nbafna/projects/mitigating-accent-bias-in-lid/wav2vec2_intermediate_outputs/vl107/wav2vec2-base-layer8-1000/cnn-attentions-linear-4/lid_model_outputs/\",\n",
    "    \"ET+duseq-train\": \"/exp/nbafna/projects/mitigating-accent-bias-in-lid/reps_phoneseqs_duseqs_exps/vl107/ecapa-tdnn_wav2vec2-xlsr-53-espeak-cv-ft_wav2vec2-base-layer8-1000/attentions-linear-4/reps-phoneseq-duseqs_lid_model_outputs\",\n",
    "    \"ET+duseqembed-train\": \"/exp/nbafna/projects/mitigating-accent-bias-in-lid/reps_phoneseqs_duseqembed_exps/vl107/ecapa-tdnn_wav2vec2-xlsr-53-espeak-cv-ft_wav2vec2-base-layer8-1000/attentions-linear-4/reps-phoneseq-duseqs_lid_model_outputs\",\n",
    "    \"phoneseqs\": \"/exp/nbafna/projects/mitigating-accent-bias-in-lid/phoneseq_exps/vl107/wav2vec2-xlsr-53-espeak-cv-ft/attentions-linear-8/phoneseq_lid_model_outputs/\",\n",
    "    \"ET+phoneseqs\": \"/exp/nbafna/projects/mitigating-accent-bias-in-lid/dists-phoneseq-systemcombo_exps/vl107/ecapa-tdnn_wav2vec2-xlsr-53-espeak-cv-ft/attentions-linear-8/lid_model_outputs/\",\n",
    "    \"ET+phoneseqs-train\": \"/exp/nbafna/projects/mitigating-accent-bias-in-lid/reps-phoneseq_exps/vl107/ecapa-tdnn_wav2vec2-xlsr-53-espeak-cv-ft/attentions-linear-8/reps-phoneseq_lid_model_outputs/\",\n",
    "}\n",
    "# datasets = [\"cv_from_hf\", \"edacc\", \"cv\", \"fleurs_test\", \"cv_from_hf_l2\"]\n",
    "datasets = [\"cv_from_hf_l2\"]\n",
    "for dataset in datasets:\n",
    "    for approach, dir in approach2dir.items():\n",
    "        print(f\"{dataset} {approach}\")\n",
    "        mean_accuracy, lower, upper = get_bootstrap_ci(dataset, dir)\n",
    "        print(f\"Mean accuracy: {mean_accuracy}\")\n",
    "        print(f\"CI: ({lower}, {upper})\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_variety, macro_avg, micro_avg, std_dev = dataset_to_processor[\"cv\"](approach2dir[\"CV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.read_edacc_results.<locals>.<lambda>()>,\n",
       "            {'sinhalese': defaultdict(int, {'total': 81, 'correct': 20}),\n",
       "             'lithuanian': defaultdict(int, {'total': 388, 'correct': 219}),\n",
       "             'bulgarian': defaultdict(int, {'correct': 44, 'total': 267}),\n",
       "             'chinese': defaultdict(int, {'correct': 239, 'total': 343}),\n",
       "             'shona': defaultdict(int, {'total': 57, 'correct': 21}),\n",
       "             'catalan': defaultdict(int, {'total': 511, 'correct': 150}),\n",
       "             'spanish': defaultdict(int, {'total': 642, 'correct': 310}),\n",
       "             'romanian': defaultdict(int, {'correct': 107, 'total': 384}),\n",
       "             'indian': defaultdict(int, {'correct': 216, 'total': 369}),\n",
       "             'colombian': defaultdict(int, {'total': 135, 'correct': 98}),\n",
       "             'nigerian': defaultdict(int, {'total': 1113, 'correct': 368}),\n",
       "             'french': defaultdict(int, {'correct': 68, 'total': 167}),\n",
       "             'mexican': defaultdict(int, {'total': 62, 'correct': 19}),\n",
       "             'pakistani': defaultdict(int, {'total': 726, 'correct': 493}),\n",
       "             'kenyan': defaultdict(int, {'total': 578, 'correct': 89}),\n",
       "             'ghanian': defaultdict(int, {'total': 240, 'correct': 35}),\n",
       "             'filipino': defaultdict(int, {'total': 73, 'correct': 17}),\n",
       "             'tagalog': defaultdict(int, {'correct': 49, 'total': 49}),\n",
       "             'jamaican': defaultdict(int, {'correct': 316, 'total': 460}),\n",
       "             'italian': defaultdict(int, {'total': 1055, 'correct': 553}),\n",
       "             'israeli': defaultdict(int, {'total': 199, 'correct': 50}),\n",
       "             'vietnamese': defaultdict(int, {'total': 955, 'correct': 296}),\n",
       "             'indonesian': defaultdict(int, {'total': 289, 'correct': 37}),\n",
       "             'polish': defaultdict(int, {'total': 195, 'correct': 149}),\n",
       "             'dutch': defaultdict(int, {'correct': 22, 'total': 45}),\n",
       "             'american': defaultdict(int, {'correct': 31, 'total': 34}),\n",
       "             'japanese': defaultdict(int, {'correct': 49, 'total': 68}),\n",
       "             'icelandic': defaultdict(int, {'correct': 96, 'total': 105}),\n",
       "             'russian': defaultdict(int, {'correct': 62, 'total': 94}),\n",
       "             'korean': defaultdict(int, {'correct': 48, 'total': 50}),\n",
       "             'ecuadorian': defaultdict(int, {'correct': 45, 'total': 51}),\n",
       "             'montenegrin': defaultdict(int, {'correct': 55, 'total': 57}),\n",
       "             'egyptian': defaultdict(int, {'correct': 404, 'total': 503}),\n",
       "             'south african': defaultdict(int, {'total': 259, 'correct': 201}),\n",
       "             'macedonian': defaultdict(int, {'correct': 69, 'total': 116}),\n",
       "             'chilean': defaultdict(int, {'correct': 86, 'total': 105}),\n",
       "             'brazilian': defaultdict(int, {'correct': 88, 'total': 106})})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_by_variety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "accent_bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
